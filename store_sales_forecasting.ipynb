{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from itertools import combinations  \n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from category_encoders import MEstimateEncoder\n",
    "from lightgbm  import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv(\"train.csv\")\n",
    "test_x = pd.read_csv(\"test.csv\")\n",
    "oil = pd.read_csv(\"oil.csv\")\n",
    "store = pd.read_csv(\"stores.csv\")\n",
    "transactions = pd.read_csv(\"transactions.csv\")\n",
    "holiday = pd.read_csv(\"holidays_events.csv\")\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep for merger \n",
    "train_x[\"dataset\"] = 0\n",
    "test_x[\"dataset\"] = 1\n",
    "\n",
    "train = pd.concat([train_x,test_x], axis=0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to merge datasets across test and train data\n",
    "def data_merge(data):\n",
    "    data = pd.merge(data,oil, on=\"date\", how=\"left\")\n",
    "    data = pd.merge(data,store, on=\"store_nbr\", how=\"left\")\n",
    "    data = pd.merge(data,transactions, on=[\"date\",\"store_nbr\"], how=\"left\")\n",
    "    data = pd.merge(data,holiday, left_on=[\"date\",\"city\"], right_on=[\"date\",\"locale_name\"], how=\"left\")\n",
    "    data = data.set_index(['store_nbr', 'date', 'family'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "train = data_merge(train)\n",
    "train = train.drop(index='2013-01-01', level=1).reset_index()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort dataset based key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values([\"store_nbr\",\"family\",\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Lag features on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "lag_features = [1,7]   # Number of lag based on lenght of prediction\n",
    "for i in lag_features:\n",
    "    train[f'lag_{i}'] = train.groupby([\"store_nbr\", \"family\"])[\"sales\"].shift(i)\n",
    "    train[f'lag_{i}'].fillna(0)   \n",
    "    train[f'transaction_lag_{i}'] = train.groupby([\"store_nbr\", \"family\"])[\"transactions\"].shift(i)\n",
    "    train[f'transaction_lag_{i}'].fillna(0)\n",
    "\n",
    "lag_features1 = [15,30,90,]   # Number of lag based on lenght of prediction\n",
    "for i in lag_features1:\n",
    "    train[f'rolling_mean_{i}'] =train.groupby([\"store_nbr\", \"family\"])[\"sales\"].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    train[f'rolling_mean_{i}'].fillna(0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting Columns types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_cat (df):\n",
    "    for column, type in zip(df.columns,df.dtypes):\n",
    "        if column == \"cluster\":\n",
    "            pass\n",
    "            #df[column] = df[column].astype(\"category\")\n",
    "        elif column == \"date\":\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\") \n",
    "        if type == \"object\" and column != \"date\":\n",
    "            df[column] = df[column].astype(\"category\")\n",
    "    return df\n",
    "\n",
    "train = object_cat(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Time Step Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_step'] = train['date'].rank(method=\"dense\", ascending=True).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate List for categorical and numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = [x for x, y in zip(train.columns, train.dtypes) if y in [\"object\", \"category\",\"bool\"] and x != \"date\"]\n",
    "num_col = [x for x, y in zip(train.columns, train.dtypes) if y not in [\"object\", \"category\",\"bool\"] and x not in [\"id\",\"date\",\"family\",\"dataset\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Train X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.set_index(\"id\")\n",
    "train_x = train[train[\"dataset\"]==0].copy()\n",
    "\n",
    "test_x = train[train[\"dataset\"]==1].copy()\n",
    "train_explore = train_x.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values and Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing oil data filled with mean of 3 days window  and others filled with zero or Unknown\n",
    "def fill_na_groups(train_xx):\n",
    "    train_xx[\"dcoilwtico\"] = train_xx.groupby([\"store_nbr\"])[\"dcoilwtico\"].transform(lambda x: x.fillna(x.rolling(3, min_periods=1).mean()))\n",
    "    train_xx[\"dcoilwtico\"] = train_xx.groupby([\"store_nbr\" ])[\"dcoilwtico\"].transform(lambda x: x.bfill())\n",
    "    train_xx[num_col] = train_xx[num_col].fillna(0)\n",
    "    \n",
    "\n",
    "    return train_xx\n",
    "\n",
    "train_x = fill_na_groups(train_x)\n",
    "test_x = fill_na_groups(test_x)\n",
    "train_explore = fill_na_groups(train_explore)\n",
    "\n",
    "train_x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[num_col].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Multi Score of our features on sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1 = train_x.copy()\n",
    "#scal = StandardScaler()\n",
    "#train_x1[lags + [\"sales\"]] =scal.fit_transform(train_x[lags + [\"sales\"]])\n",
    "#train_x1[lags + [\"sales\"]] = train_x1[lags + [\"sales\"]].fillna(0)\n",
    "sales =train_x1.pop(\"sales\")\n",
    "for colname in train_x1[cat_col].select_dtypes([\"object\",\"category\"]):\n",
    "    train_x1[colname], _ = train_x1[colname].factorize()\n",
    "\n",
    "for colname in train_x1.select_dtypes([\"datetime\"]):\n",
    "    train_x1[colname] = train_x1[colname].astype(\"int64\") // 10**9  # Convert to seconds since epoch\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = train_x1.dtypes == np.int64\n",
    "discrete_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(train_x1, sales, discrete_features=discrete_features)\n",
    "print(mi_scores[::3])  # show a few features with their MI scores\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores[:] #  show all to determine desired cut off point below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores1 = pd.Series(mi_scores, index=train_x.columns)\n",
    "selected_columns = train_x.columns[(mi_scores1 > 0.01)].tolist() \n",
    "\n",
    "# Remove the 'transactions' column if it exists as will cause data leakeage in future predictions given its unknown\n",
    "selected_columns = [col for col in selected_columns if col != 'transactions']\n",
    "train_x.loc[:,selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Numeric Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram and Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore1 = train_explore[train_explore[\"date\"] <=\"2024-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Plot and Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(16, 5),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot('time_step', 'sales',data=train_explore[num_col], color='0.75')\n",
    "ax =sns.regplot(x='time_step', y='sales', data=train_explore[num_col], ci=None, scatter_kws=dict(color='0.25'),)\n",
    "ax.set_title(f'Time Plot of Sales ');\n",
    "\n",
    "# Plot\n",
    "train_explore[\"date_ordinal\"] = train_explore[\"date\"].map(mdates.date2num)\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "sns.regplot(x=\"date_ordinal\", y=\"sales\", data=train_explore, ci=None, scatter_kws={\"color\": \"0.25\"}, ax=ax)\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))  # Outer level: Every year\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())  # Inner level: Every month\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"\\n%Y\"))  # Year with newline\n",
    "ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%b\"))  # Month (Jan, Feb, etc.)\n",
    "\n",
    "# Convert back to datetime scale\n",
    "#ax.set_xticks(train_explore[\"date_ordinal\"][::3])  # Adjust tick density\n",
    "#ax.set_xticklabels(train_explore[\"date\"].dt.strftime(\"%b\\n%Y\")[::3])  \n",
    "\n",
    "# Rotate month labels\n",
    "plt.setp(ax.get_xticklabels(minor=True), rotation=45, ha=\"right\")\n",
    "# Improve readability\n",
    "plt.xticks(rotation=0)  # Rotate if needed\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales Trend Over Time\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(9,6, figsize=(60,40))\n",
    "ax = ax.flatten()\n",
    "for axs, store in zip(ax,train_explore[\"store_nbr\"].unique()):\n",
    "    train_explore2 = train_explore[train_explore[\"store_nbr\"] == store]\n",
    "    axs.plot('time_step', 'sales',data=train_explore2[num_col], color='0.5')\n",
    "    sns.regplot(x='time_step', y='sales', data=train_explore2[num_col], ci=None, scatter_kws=dict(color='0.25'), ax=axs)\n",
    "    axs.set_title(f'Time Plot of Sales store_nbr {store}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The time plot, highlights that for most stores , over the years their max sales increase, which shows the importance of time as a factor. Further analysis will be subsequently provided based on monthly turnovers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lag Plot on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = sns.regplot(x='lag_7', y='sales', data=train_explore, ci=None, scatter_kws=dict(color='0.25'))\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Lag Plot of Sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking at the overall lag plot across all sales, no relationship can be established between previous days sales on current day sales, we would explore further like on the time step plot based on individual stores number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(9,6, figsize=(60,40))\n",
    "ax = ax.flatten()\n",
    "for axs, store in zip(ax,train_explore[\"store_nbr\"].unique()):\n",
    "    train_explore2 = train_explore[train_explore[\"store_nbr\"] == store]\n",
    "    sns.regplot(x='lag_7', y='sales', data=train_explore2[num_col], ci=None, scatter_kws=dict(color='0.25'), ax=axs)\n",
    "    axs.set_title(f'Time Plot of Sales store_nbr {store}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the Individual plots, we can see a more valid serial dependence, that previous day sales affect current day. Further analysis can be done at yearly, monthly or weekly level based on average. However this is not currently available in this version of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore2 = train_explore[train_explore[\"store_nbr\"] == 5].set_index(\"date\")#.to_period()\n",
    "moving_average = train_explore2[\"sales\"].rolling(\n",
    "    window=365,       # 365-day window\n",
    "    center=True,      # puts the average at the center of the window\n",
    "    min_periods=183,  # choose about half the window size\n",
    ").mean()              # compute the mean (could also do median, std, min, max, ...)\n",
    "\n",
    "ax = train_explore2[\"sales\"].plot(style=\".\", color=\"0.5\")\n",
    "moving_average.plot(\n",
    "    ax=ax, linewidth=3, title=\"Sales Prediction - 365-Day Moving Average\", legend=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Line plot of Sales Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(num_col) #display(cat_col).\n",
    "train_explore[\"month_day\"] = train_explore[\"date\"].dt.strftime(\"%m-%d\") \n",
    "fig, axes = plt.subplots(6, 1, figsize=(20,5*6), constrained_layout=True)\n",
    "\n",
    "for ax, x in zip(axes,train_explore[[col for col in num_col if col != \"time_step\"]].columns) :\n",
    "    #sns.histplot(x=train_explore[x], ax=ax[0], color=\"green\", bins=30, kde=True)\n",
    "    #ax[0].set_title(f\"Histogram of {x}\")\n",
    "    #ax[0].tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    if x == \"store_nbr\":\n",
    "        sns.lineplot(x=\"month_day\", y=\"sales\", ax=ax,alpha=0.8, palette=\"coolwarm\", hue =\"store_nbr\",\n",
    "        data=train_explore.loc[train_explore[\"sales\"] != 0].groupby([\"month_day\",\"store_nbr\"])[\"sales\"].median().reset_index(), )\n",
    "        ax.set_title(f\"Lineplot of Median {x} based on month and store \")\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator())  # Show one tick per month\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n",
    "        ax.tick_params(axis='both', labelsize=20)\n",
    "    elif x ==\"cluster\":\n",
    "        pass\n",
    "    else:\n",
    "        sns.lineplot(x=\"month_day\", y=x, ax=ax,alpha=0.8, palette=\"coolwarm\", hue =\"store_nbr\",\n",
    "        data=train_explore.loc[train_explore[x] != 0].groupby([\"month_day\",\"store_nbr\"])[x].median().reset_index(), )\n",
    "        ax.set_title(f\"Lineplot of (Median) {x} based on month and store\")\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator())  # Show one tick per month\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n",
    "        ax.tick_params(axis='both', labelsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore[\"month\"] = train_explore[\"date\"].dt.strftime(\"%m\") \n",
    "train_explore2 = train_explore[train_explore[\"date\"] <= \"2013-01-15\"]\n",
    "#sns.pairplot(train_explore2[[\"sales\"] + num_col], kind = 'reg', diag_kind=\"kde\", hue =\"store_nbr\",\n",
    " #x_vars=[\"sales\",\"transactions\",\"dcoilwtico\"] ,y_vars=[\"sales\",\"transactions\",\"dcoilwtico\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of Top 10  Performing Store in terms of Total Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sales = pd.DataFrame(train_explore.loc[train_explore[\"sales\"] != 0].groupby([\"store_nbr\"])[\"sales\"].sum().reset_index())\n",
    "top10= train_sales.sort_values(\"sales\", ascending=False)[:10]\n",
    "top10stores=top10[\"store_nbr\"].to_numpy() \n",
    "train_top10 = train_explore[train_explore[\"store_nbr\"].isin(top10stores)]\n",
    "sorted(top10stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore2 = train_top10 #[train_top10[\"date\"] <= \"2020-01-31\"]\n",
    "\n",
    "# Split top 10 stores into two sets\n",
    "top5_stores = train_top10[\"month\"].unique()\n",
    "\n",
    "# Create a single figure with subplots (2 columns for side-by-side placement)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(18, 25), sharex=True, sharey=True, constrained_layout=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, store in enumerate(top5_stores):\n",
    "    sns.kdeplot(data=train_explore2[train_explore2[\"month\"] == store], \n",
    "                x=\"sales\", hue=\"store_nbr\", ax=axes[i],)\n",
    "    axes[i].set_title(f\"Month {store}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The plot shows sales distributions by store across all 12 months. Sales are highly right-skewed with most values clustered near zero and occasional high outliers. The distribution shape is consistent, suggesting stable seasonality with some store-specific spikes. This might call the need to do a log transformation on sales to reduce skewness (np.log1p) and reconversion (np.expm1) post analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single figure with subplots (2 columns for side-by-side placement)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(18, 25), sharex=True, sharey=True, constrained_layout=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "train_explore2[\"sales_log\"] = np.log1p(train_explore2[\"sales\"])\n",
    "\n",
    "for i, store in enumerate(top5_stores):\n",
    "    sns.kdeplot(data=train_explore2[train_explore2[\"month\"] == store], \n",
    "                x=\"sales_log\", hue=\"store_nbr\", ax=axes[i],)\n",
    "    axes[i].set_title(f\"Month {store}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### After log transformation (`sales_log`), the sales distributions across stores and months appear more symmetric and multi-modal, with reduced skewness. Peaks are better aligned across stores, indicating improved comparability and stabilized variance ideal for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore3 = train_top10[(train_top10[\"date\"] <= \"2014-01-31\") & (train_top10[\"sales\"]>1) & (train_top10[\"sales\"]<=1000)]\n",
    "# Create a single figure with subplots (2 columns for side-by-side placement)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(12, 15), sharex=True, sharey=True, constrained_layout=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, store in enumerate(top5_stores):\n",
    "    sns.boxplot(data=train_explore3[train_explore3[\"month\"] == store], \n",
    "                x=\"month\",y=\"sales\", hue=\"store_nbr\", ax=axes[i])\n",
    "    axes[i].set_title(f\"Month {store}\")\n",
    "    axes[i].legend().remove()   # dont display individual legend\n",
    "\n",
    "handles, labels = axes[1].get_legend_handles_labels()  # Get legend from one subplot\n",
    "fig.legend(handles, labels, title=\"Store\",loc=\"upper left\", ncol=4, fontsize=6)\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This boxplot shows monthly sales distributions across multiple stores. Key points:\n",
    "\n",
    "- **Sales values are fairly consistent** across months and stores, with median sales mostly between 200–400.\n",
    "- All months show **significant outliers**, especially beyond 800, indicating sporadic high sales days.\n",
    "- Some stores (e.g. Store 8 and Store 50) consistently show **higher medians** than others, suggesting stronger performance.\n",
    "- Variance is generally stable across months, showing no clear seasonal spike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_transform(df):\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"day\"] = df[\"date\"].dt.day\n",
    "    df[\"day_of_week\"] = df[\"date\"].dt.weekday\n",
    "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)  # 1 if Sat\n",
    "    df = df.drop(columns=[\"date\"])\n",
    "    return df\n",
    "\n",
    "train_x = date_transform(train_x)\n",
    "test_x = date_transform(test_x)\n",
    "\n",
    "\n",
    "selected_columns = [col for col in selected_columns if col not in ['transactions','date']] + [\"year\",\"month\",\"day\",\"day_of_week\",\"is_weekend\"]\n",
    "train_x.loc[:,selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Interaction term with date feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"day\", \"day_of_week\", \"lag_7\"]  \n",
    "data = [train_x,test_x]\n",
    "for tl in data: \n",
    "    for col1, col2 in combinations(cols, 2):  \n",
    "        tl[f\"{col1}_x_{col2}\"] = tl[col1] * tl[col2]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create of Log of Sales to reduce Skewness in Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[\"sales_log\"] = np.log1p(train_x[\"sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get List of Numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train_x.select_dtypes(include='number').columns.tolist()\n",
    "cr = train_x[numeric_cols].corr()\n",
    "cr[\"sales_log\"].sort_values(ascending=False)[:4] #list top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use time_step for split\n",
    "split_point = train_x[\"time_step\"].max() * 0.8  # 80% of total time range\n",
    "x_train = train_x[train_x[\"time_step\"] <= split_point]\n",
    "x_test = train_x[train_x[\"time_step\"] > split_point]\n",
    "\n",
    "y_train = x_train[\"sales\"].copy()\n",
    "y_test = x_test[\"sales\"].copy() \n",
    "selected_columns = [col for col in selected_columns if col != \"time_step\"]\n",
    "x_train = x_train.loc[:,selected_columns]   #[features].drop([\"sales\",\"dataset\"], axis=\"columns\")\n",
    "x_test = x_test.loc[:,selected_columns]     #[features].drop([\"sales\",\"dataset\"], axis=\"columns\")\n",
    "\n",
    "\n",
    "# Verify shapes\n",
    "print(len(y_train), len(y_test), len(x_train), len(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify Category columns for Model Choice below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = x_train.select_dtypes(include=[\"category\"]).columns.tolist()\n",
    "cat_indices = [x_train.columns.get_loc(col) for col in categorical_cols]\n",
    "print(\"Indices\",cat_indices,\"Categorical columns:X_train\", categorical_cols,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder instance. Choose m to control noise.\n",
    "encoder = MEstimateEncoder(cols=categorical_cols, m=5.0)\n",
    "\n",
    "# Fit the encoder on the encoding split.\n",
    "encoder.fit(x_train, y_train)\n",
    "\n",
    "x_train = encoder.transform(x_train)\n",
    "x_test = encoder.transform(x_test)\n",
    "test_x = encoder.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log1p(y_train.copy())\n",
    "y_test_log = np.log1p(y_test.copy())  #np.log1p\n",
    "\n",
    "\n",
    "column_sets = {\n",
    "    \"lags_1\": [col for col in x_train.columns if col not in [\"lag_7\", \"transaction_lag_7\"]],\n",
    "    \"lags_7\": [col for col in x_train.columns if col not in [\"lag_1\", \"transaction_lag_1\" ]]\n",
    "}\n",
    "\n",
    "\n",
    "param_grid = list(zip(\n",
    "    [1500],   # n_estimators\n",
    "    [0.01],   # learning_rate\n",
    "    [3],     # max_depth\n",
    "    [1500],   # min_data_in_leaf\n",
    "    [128]     # num_leaves\n",
    "))\n",
    "\n",
    "models = {}\n",
    "\n",
    "for name, selected_columns in column_sets.items():\n",
    "\n",
    "    X_train_sel = x_train[selected_columns]\n",
    "    X_test_sel = x_test[selected_columns]\n",
    "\n",
    "    model_name = f\"LGBM_{name}\"\n",
    "\n",
    "    for n_est, lr, max_d, lef, pop in param_grid:\n",
    "\n",
    "        name = LGBMRegressor(\n",
    "            n_estimators=n_est,\n",
    "            objective=\"regression\",\n",
    "            min_data_in_leaf=lef,\n",
    "            num_leaves=pop,\n",
    "            min_split_gain=0.05,\n",
    "            learning_rate=lr,\n",
    "            #max_depth=max_d,\n",
    "            lambda_l1 =1.5,\n",
    "            lambda_l2=3.5,\n",
    "            device=\"gpu\",\n",
    "            gpu_platform_id=1,\n",
    "            gpu_device_id=0,\n",
    "            verbose=-1,\n",
    "            metric=\"rmsle\"\n",
    "        )\n",
    "\n",
    "        name.fit(\n",
    "            X_train_sel, y_train_log,\n",
    "            eval_set=[(X_test_sel, y_test_log)],\n",
    "            eval_metric='rmsle',\n",
    "            categorical_feature=cat_indices,\n",
    "        )\n",
    "\n",
    "        # Save model object\n",
    "        models[model_name] = name\n",
    "\n",
    "        # Save selected columns used by the model\n",
    "        models[f\"{model_name}_columns\"] = selected_columns\n",
    "\n",
    "        print(f\"Trained and saved model: {model_name}\")\n",
    "\n",
    "        y_pred = name.predict(X_test_sel,)\n",
    "        y_pred1 = name.predict(X_train_sel,)\n",
    "        #Clip negative predictions to 0\n",
    "        y_pred_actual = np.expm1(y_pred)  # invert before reporting\n",
    "        y_pred1_actual = np.expm1(y_pred1)\n",
    "\n",
    "        y_pred_actual = np.maximum(y_pred_actual,0)  # invert before reporting\n",
    "        y_pred1_actual = np.maximum(y_pred1_actual,0)\n",
    "\n",
    "        \n",
    "\n",
    "        y_pred = y_pred_actual\n",
    "        y_pred1 = y_pred1_actual\n",
    "        rmsle_score = np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
    "        rmsle_score1 = np.sqrt(mean_squared_log_error(y_train, y_pred1))\n",
    "\n",
    "        print(f\"Model: {name} | n_estimators: {n_est} | LR: {lr} | num_leaves: {pop} | Train RMSLE: {rmsle_score1:.4f} | Test RMSLE: {rmsle_score:.4f}\")\n",
    "# #4626 #4571  #1917 #4456 #4370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models\n",
    "models_res ={}\n",
    "# Predict directly\n",
    "\n",
    "for name, selected_columns in column_sets.items():\n",
    "    test_x= test_x[selected_columns]\n",
    "    clap = np.maximum(np.expm1(models[(\"LGBM_\"+name)].predict(test_x)),0)\n",
    "    models_res[name] = clap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Ensembling based on two models based on lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_res[\"lags_average\"] = np.round(((models_res[\"lags_1\"] + models_res[\"lags_7\"] )/2),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({\"idx\":test_x.index,\"model_lag_1\":models_res[\"lags_1\"],\n",
    "                    \"model_lag_7\": models_res[\"lags_7\"],\n",
    "                    \"model_average\": models_res[\"lags_average\"]})\n",
    "\n",
    "res = res.sort_values(\"idx\")\n",
    "res.head(10)  # View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_res = pd.Series(y_train - y_pred1)\n",
    "y_test_res = pd.Series(y_test - y_pred)\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=y_test, y=y_test_res, alpha=0.6)\n",
    "plt.axhline(0, color='red', linestyle='--')  # Reference line at y=0\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals (y_true - y_pred)\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "importance_xgb = models[(\"LGBM_\"+name)].feature_importances_\n",
    "sorted_idx = np.argsort(importance_xgb)[::-1]\n",
    "features1 = x_train[selected_columns].columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh([features1[i] for i in sorted_idx], importance_xgb[sorted_idx], color=\"greenyellow\")\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Light GBM Regression Feature Importance')\n",
    "plt.gca().invert_yaxis()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({\"id\":test_x.index,\n",
    "                    \"sales\": models_res[\"lags_average\"]})\n",
    "\n",
    "res = res.sort_values(\"id\")\n",
    "res.to_csv('submission.csv', index=False)\n",
    "res.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
