{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import LightGBMModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import rmse\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"C:\\Users\\hp\\Downloads\\playground-series-s5e2\\store-sales-time-series-forecasting\\submission.csv\"\n",
    "train_x = pd.read_csv(\"train.csv\")\n",
    "test_x = pd.read_csv(\"test.csv\")\n",
    "oil = pd.read_csv(\"oil.csv\")\n",
    "store = pd.read_csv(\"stores.csv\")\n",
    "transactions = pd.read_csv(\"transactions.csv\")\n",
    "holiday = pd.read_csv(\"holidays_events.csv\")\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep for merger \n",
    "train_x[\"dataset\"] = 0\n",
    "test_x[\"dataset\"] = 1\n",
    "\n",
    "train = pd.concat([train_x,test_x], axis=0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to merge datasets across test and train data\n",
    "def data_merge(data):\n",
    "    data = pd.merge(data,oil, on=\"date\", how=\"left\")\n",
    "    data = pd.merge(data,store, on=\"store_nbr\", how=\"left\")\n",
    "    data = pd.merge(data,transactions, on=[\"date\",\"store_nbr\"], how=\"left\")\n",
    "    data = pd.merge(data,holiday, left_on=[\"date\",\"city\"], right_on=[\"date\",\"locale_name\"], how=\"left\")\n",
    "    data = data.set_index(['store_nbr', 'date', 'family'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "train = data_merge(train)\n",
    "train = train.drop(index='2013-01-01', level=1).reset_index()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort dataset based key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values([\"store_nbr\",\"family\",\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Lag features on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "lag_features = [1,7]   # Number of lag based on lenght of prediction\n",
    "for i in lag_features:\n",
    "    train[f'lag_{i}'] = train.groupby([\"store_nbr\", \"family\"])[\"sales\"].shift(i)\n",
    "    train[f'lag_{i}'].fillna(0)   \n",
    "    train[f'transaction_lag_{i}'] = train.groupby([\"store_nbr\", \"family\"])[\"transactions\"].shift(i)\n",
    "    train[f'transaction_lag_{i}'].fillna(0)\n",
    "\n",
    "lag_features1 = [15,30,90,]   # Number of lag based on lenght of prediction\n",
    "for i in lag_features1:\n",
    "    train[f'rolling_mean_{i}'] =train.groupby([\"store_nbr\", \"family\"])[\"sales\"].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    train[f'rolling_mean_{i}'].fillna(0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting Columns types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_cat (df):\n",
    "    for column, type in zip(df.columns,df.dtypes):\n",
    "        if column == \"cluster\":\n",
    "            pass\n",
    "            #df[column] = df[column].astype(\"category\")\n",
    "        elif column == \"date\":\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\") \n",
    "        if type == \"object\" and column != \"date\":\n",
    "            df[column] = df[column].astype(\"category\")\n",
    "    return df\n",
    "\n",
    "train = object_cat(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Time Step Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_step'] = train['date'].rank(method=\"dense\", ascending=True).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate List for categorical and numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = [x for x, y in zip(train.columns, train.dtypes) if y in [\"object\", \"category\",\"bool\"] and x != \"date\"]\n",
    "num_col = [x for x, y in zip(train.columns, train.dtypes) if y not in [\"object\", \"category\",\"bool\"] and x not in [\"id\",\"date\",\"family\",\"dataset\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Train X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.set_index(\"id\")\n",
    "train_x = train[train[\"dataset\"]==0].copy()\n",
    "\n",
    "test_x = train[train[\"dataset\"]==1].copy()\n",
    "train_explore = train_x.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values and Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing oil data filled with mean of 3 days window  and others filled with zero or Unknown\n",
    "def fill_na_groups(train_xx):\n",
    "    train_xx[\"dcoilwtico\"] = train_xx.groupby([\"store_nbr\"])[\"dcoilwtico\"].transform(lambda x: x.fillna(x.rolling(3, min_periods=1).mean()))\n",
    "    train_xx[\"dcoilwtico\"] = train_xx.groupby([\"store_nbr\" ])[\"dcoilwtico\"].transform(lambda x: x.bfill())\n",
    "    train_xx[num_col] = train_xx[num_col].fillna(0)\n",
    "    \n",
    "\n",
    "    return train_xx\n",
    "\n",
    "train_x = fill_na_groups(train_x)\n",
    "test_x = fill_na_groups(test_x)\n",
    "train_explore = fill_na_groups(train_explore)\n",
    "\n",
    "train_x.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Numeric Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram and Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore1 = train_explore[train_explore[\"date\"] <=\"2024-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Plot and Regression Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(16, 5),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot('time_step', 'sales',data=train_explore[num_col], color='0.75')\n",
    "ax =sns.regplot(x='time_step', y='sales', data=train_explore[num_col], ci=None, scatter_kws=dict(color='0.25'),)\n",
    "ax.set_title(f'Time Plot of Sales ');\n",
    "\n",
    "# Plot\n",
    "train_explore[\"date_ordinal\"] = train_explore[\"date\"].map(mdates.date2num)\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "sns.regplot(x=\"date_ordinal\", y=\"sales\", data=train_explore, ci=None, scatter_kws={\"color\": \"0.25\"}, ax=ax)\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))  # Outer level: Every year\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())  # Inner level: Every month\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"\\n%Y\"))  # Year with newline\n",
    "ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%b\"))  # Month (Jan, Feb, etc.)\n",
    "\n",
    "# Convert back to datetime scale\n",
    "#ax.set_xticks(train_explore[\"date_ordinal\"][::3])  # Adjust tick density\n",
    "#ax.set_xticklabels(train_explore[\"date\"].dt.strftime(\"%b\\n%Y\")[::3])  \n",
    "\n",
    "# Rotate month labels\n",
    "plt.setp(ax.get_xticklabels(minor=True), rotation=45, ha=\"right\")\n",
    "# Improve readability\n",
    "plt.xticks(rotation=0)  # Rotate if needed\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Sales Trend Over Time\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(9,6, figsize=(60,40))\n",
    "ax = ax.flatten()\n",
    "for axs, store in zip(ax,train_explore[\"store_nbr\"].unique()):\n",
    "    train_explore2 = train_explore[train_explore[\"store_nbr\"] == store]\n",
    "    axs.plot('time_step', 'sales',data=train_explore2[num_col], color='0.5')\n",
    "    sns.regplot(x='time_step', y='sales', data=train_explore2[num_col], ci=None, scatter_kws=dict(color='0.25'), ax=axs)\n",
    "    axs.set_title(f'Time Plot of Sales store_nbr {store}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The time plot, highlights that for most stores , over the years their max sales increase, which shows the importance of time as a factor. Further analysis will be subsequently provided based on monthly turnovers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lag Plot on Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = sns.regplot(x='lag_7', y='sales', data=train_explore, ci=None, scatter_kws=dict(color='0.25'))\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Lag Plot of Sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking at the overall lag plot across all sales, no relationship can be established between previous days sales on current day sales, we would explore further like on the time step plot based on individual stores number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(9,6, figsize=(60,40))\n",
    "ax = ax.flatten()\n",
    "for axs, store in zip(ax,train_explore[\"store_nbr\"].unique()):\n",
    "    train_explore2 = train_explore[train_explore[\"store_nbr\"] == store]\n",
    "    sns.regplot(x='lag_7', y='sales', data=train_explore2[num_col], ci=None, scatter_kws=dict(color='0.25'), ax=axs)\n",
    "    axs.set_title(f'Time Plot of Sales store_nbr {store}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the Individual plots, we can see a more valid serial dependence, that previous day sales affect current day. Further analysis can be done at yearly, monthly or weekly level based on average. However this is not currently available in this version of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore2 = train_explore[train_explore[\"store_nbr\"] == 5].set_index(\"date\")#.to_period()\n",
    "moving_average = train_explore2[\"sales\"].rolling(\n",
    "    window=365,       # 365-day window\n",
    "    center=True,      # puts the average at the center of the window\n",
    "    min_periods=183,  # choose about half the window size\n",
    ").mean()              # compute the mean (could also do median, std, min, max, ...)\n",
    "\n",
    "ax = train_explore2[\"sales\"].plot(style=\".\", color=\"0.5\")\n",
    "moving_average.plot(\n",
    "    ax=ax, linewidth=3, title=\"Sales Prediction - 365-Day Moving Average\", legend=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Line plot of Sales Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(num_col) #display(cat_col).\n",
    "train_explore[\"month_day\"] = train_explore[\"date\"].dt.strftime(\"%m-%d\") \n",
    "fig, axes = plt.subplots(6, 1, figsize=(20,5*6), constrained_layout=True)\n",
    "\n",
    "for ax, x in zip(axes,train_explore[[col for col in num_col if col != \"time_step\"]].columns) :\n",
    "    #sns.histplot(x=train_explore[x], ax=ax[0], color=\"green\", bins=30, kde=True)\n",
    "    #ax[0].set_title(f\"Histogram of {x}\")\n",
    "    #ax[0].tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    if x == \"store_nbr\":\n",
    "        sns.lineplot(x=\"month_day\", y=\"sales\", ax=ax,alpha=0.8, palette=\"coolwarm\", hue =\"store_nbr\",\n",
    "        data=train_explore.loc[train_explore[\"sales\"] != 0].groupby([\"month_day\",\"store_nbr\"])[\"sales\"].median().reset_index(), )\n",
    "        ax.set_title(f\"Lineplot of Median {x} based on month and store \")\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator())  # Show one tick per month\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n",
    "        ax.tick_params(axis='both', labelsize=20)\n",
    "    elif x ==\"cluster\":\n",
    "        pass\n",
    "    else:\n",
    "        sns.lineplot(x=\"month_day\", y=x, ax=ax,alpha=0.8, palette=\"coolwarm\", hue =\"store_nbr\",\n",
    "        data=train_explore.loc[train_explore[x] != 0].groupby([\"month_day\",\"store_nbr\"])[x].median().reset_index(), )\n",
    "        ax.set_title(f\"Lineplot of (Median) {x} based on month and store\")\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator())  # Show one tick per month\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n",
    "        ax.tick_params(axis='both', labelsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore[\"month\"] = train_explore[\"date\"].dt.strftime(\"%m\") \n",
    "train_explore2 = train_explore[train_explore[\"date\"] <= \"2013-01-15\"]\n",
    "#sns.pairplot(train_explore2[[\"sales\"] + num_col], kind = 'reg', diag_kind=\"kde\", hue =\"store_nbr\",\n",
    " #x_vars=[\"sales\",\"transactions\",\"dcoilwtico\"] ,y_vars=[\"sales\",\"transactions\",\"dcoilwtico\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of Top 10  Performing Store in terms of Total Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sales = pd.DataFrame(train_explore.loc[train_explore[\"sales\"] != 0].groupby([\"store_nbr\"])[\"sales\"].sum().reset_index())\n",
    "top10= train_sales.sort_values(\"sales\", ascending=False)[:10]\n",
    "top10stores=top10[\"store_nbr\"].to_numpy() \n",
    "train_top10 = train_explore[train_explore[\"store_nbr\"].isin(top10stores)]\n",
    "sorted(top10stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore2 = train_top10 #[train_top10[\"date\"] <= \"2020-01-31\"]\n",
    "\n",
    "# Split top 10 stores into two sets\n",
    "top5_stores = train_top10[\"month\"].unique()\n",
    "\n",
    "# Create a single figure with subplots (2 columns for side-by-side placement)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(18, 25), sharex=True, sharey=True, constrained_layout=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, store in enumerate(top5_stores):\n",
    "    sns.kdeplot(data=train_explore2[train_explore2[\"month\"] == store], \n",
    "                x=\"sales\", hue=\"store_nbr\", ax=axes[i],)\n",
    "    axes[i].set_title(f\"Month {store}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The plot shows sales distributions by store across all 12 months. Sales are highly right-skewed with most values clustered near zero and occasional high outliers. The distribution shape is consistent, suggesting stable seasonality with some store-specific spikes. This might call the need to do a log transformation on sales to reduce skewness (np.log1p) and reconversion (np.expm1) post analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single figure with subplots (2 columns for side-by-side placement)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(18, 25), sharex=True, sharey=True, constrained_layout=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "train_explore2[\"sales_log\"] = np.log1p(train_explore2[\"sales\"])\n",
    "\n",
    "for i, store in enumerate(top5_stores):\n",
    "    sns.kdeplot(data=train_explore2[train_explore2[\"month\"] == store], \n",
    "                x=\"sales_log\", hue=\"store_nbr\", ax=axes[i],)\n",
    "    axes[i].set_title(f\"Month {store}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### After log transformation (`sales_log`), the sales distributions across stores and months appear more symmetric and multi-modal, with reduced skewness. Peaks are better aligned across stores, indicating improved comparability and stabilized variance ideal for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explore3 = train_top10[(train_top10[\"date\"] <= \"2014-01-31\") & (train_top10[\"sales\"]>1) & (train_top10[\"sales\"]<=1000)]\n",
    "# Create a single figure with subplots (2 columns for side-by-side placement)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(12, 15), sharex=True, sharey=True, constrained_layout=True)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, store in enumerate(top5_stores):\n",
    "    sns.boxplot(data=train_explore3[train_explore3[\"month\"] == store], \n",
    "                x=\"month\",y=\"sales\", hue=\"store_nbr\", ax=axes[i])\n",
    "    axes[i].set_title(f\"Month {store}\")\n",
    "    axes[i].legend().remove()   # dont display individual legend\n",
    "\n",
    "handles, labels = axes[1].get_legend_handles_labels()  # Get legend from one subplot\n",
    "fig.legend(handles, labels, title=\"Store\",loc=\"upper left\", ncol=4, fontsize=6)\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This boxplot shows monthly sales distributions across multiple stores. Key points:\n",
    "\n",
    "- **Sales values are fairly consistent** across months and stores, with median sales mostly between 200–400.\n",
    "- All months show **significant outliers**, especially beyond 800, indicating sporadic high sales days.\n",
    "- Some stores (e.g. Store 8 and Store 50) consistently show **higher medians** than others, suggesting stronger performance.\n",
    "- Variance is generally stable across months, showing no clear seasonal spike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darts ANalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x = train_x[train_x[\"store_nbr\"] <= 3]\n",
    "#test_x = test_x[test_x[\"store_nbr\"] <= 3]\n",
    "\n",
    "train_x['date'] = pd.to_datetime(train_x['date'])\n",
    "test_x['date'] = pd.to_datetime(test_x['date'])\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "for (store, family), group_train in train_x.groupby([\"store_nbr\", \"family\"]):\n",
    "    group_test = test_x[(test_x[\"store_nbr\"] == store) & (test_x[\"family\"] == family)]\n",
    "\n",
    "    full_df = pd.concat([\n",
    "        group_train[['date', 'sales']],\n",
    "        group_test[['date']].assign(sales=np.nan)\n",
    "    ])\n",
    "\n",
    "    # Remove duplicates by date (if any)\n",
    "    full_df = full_df.drop_duplicates(subset='date', keep='first')\n",
    "\n",
    "    try:\n",
    "        series = TimeSeries.from_dataframe(\n",
    "            full_df,\n",
    "            time_col='date',\n",
    "            value_cols='sales',\n",
    "            fill_missing_dates=True,\n",
    "            freq=\"D\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping store {store}, family {family} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    scaler = Scaler()\n",
    "    series_log = series.map(np.log1p)\n",
    "    series_scaled = scaler.fit_transform(series_log)\n",
    "\n",
    "\n",
    "    # Split using the last known date\n",
    "    split_point = group_train['date'].max()\n",
    "    train_series = series_scaled.drop_after(split_point)\n",
    "    train_series = train_series.with_values(np.nan_to_num(train_series.values(), nan=0.0))\n",
    "\n",
    "\n",
    "\n",
    "    model = LightGBMModel(lags=14, output_chunk_length=len(group_test),\n",
    "                          device=\"gpu\",\n",
    "                          gpu_device_id=0,\n",
    "                          gpu_platform_id=1,\n",
    "        )\n",
    "    model.fit(train_series)\n",
    "\n",
    "    forecast = model.predict(len(group_test))\n",
    "    forecast_final = scaler.inverse_transform(forecast).map(np.expm1)\n",
    "\n",
    "\n",
    "    # Store with identifying columns\n",
    "    df_preds = group_test[['store_nbr', 'family', 'date']].copy()\n",
    "    df_preds['predicted_sales'] = forecast_final.values().flatten()\n",
    "    all_preds.append(df_preds)\n",
    "\n",
    "# Final predictions\n",
    "predictions_df = pd.concat(all_preds).sort_values(['store_nbr', 'family', 'date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({#\"idx\":predictions_df.index,\n",
    "                    \"sales\": predictions_df[\"predicted_sales\"]})\n",
    "\n",
    "res = res.reset_index()\n",
    "res = res.sort_values(\"id\")\n",
    "res.to_csv('submission.csv', index=False)\n",
    "res.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
